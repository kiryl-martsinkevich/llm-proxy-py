# LLM Router Configuration for Claude Code → Ollama
#
# This configuration routes all Claude Code requests to a local Ollama instance
# running the gpt-oss:20b model. This allows you to use Claude Code with local
# models instead of cloud providers.
#
# Setup:
# 1. Install and start Ollama: https://ollama.ai
# 2. Pull the gpt-oss:20b model: ollama pull gpt-oss:20b
# 3. Start this router: uv run llm-router --config config.claude-code-ollama.yaml
# 4. Configure Claude Code to use http://localhost:8000 as the API endpoint

# Server configuration
server:
  host: "0.0.0.0"
  port: 8000
  log_requests: true        # Enable to see what Claude Code is sending
  log_responses: false      # Disable to avoid logging large responses
  mask_api_keys: true       # Mask API keys in logs

# Model configurations
# Map all common Claude Code model requests to local Ollama
models:
  # Claude models → Ollama gpt-oss:20b
  claude-3-5-sonnet-20241022:
    provider: ollama
    endpoint: "http://localhost:11434"
    actual_model_name: "gpt-oss:20b"
    timeout: 120.0
    connect_timeout: 5.0
    ssl_verify: false

  claude-3-5-sonnet-20240620:
    provider: ollama
    endpoint: "http://localhost:11434"
    actual_model_name: "gpt-oss:20b"
    timeout: 120.0
    connect_timeout: 5.0
    ssl_verify: false

  claude-3-opus-20240229:
    provider: ollama
    endpoint: "http://localhost:11434"
    actual_model_name: "gpt-oss:20b"
    timeout: 120.0
    connect_timeout: 5.0
    ssl_verify: false

  claude-3-sonnet-20240229:
    provider: ollama
    endpoint: "http://localhost:11434"
    actual_model_name: "gpt-oss:20b"
    timeout: 120.0
    connect_timeout: 5.0
    ssl_verify: false

  claude-3-haiku-20240307:
    provider: ollama
    endpoint: "http://localhost:11434"
    actual_model_name: "gpt-oss:20b"
    timeout: 120.0
    connect_timeout: 5.0
    ssl_verify: false

  # Generic Claude aliases
  claude-3-opus:
    provider: ollama
    endpoint: "http://localhost:11434"
    actual_model_name: "gpt-oss:20b"
    timeout: 120.0
    ssl_verify: false

  claude-3-sonnet:
    provider: ollama
    endpoint: "http://localhost:11434"
    actual_model_name: "gpt-oss:20b"
    timeout: 120.0
    ssl_verify: false

  claude-3-haiku:
    provider: ollama
    endpoint: "http://localhost:11434"
    actual_model_name: "gpt-oss:20b"
    timeout: 120.0
    ssl_verify: false

  # OpenAI models → Ollama gpt-oss:20b (in case Claude Code uses these)
  gpt-4:
    provider: ollama
    endpoint: "http://localhost:11434"
    actual_model_name: "gpt-oss:20b"
    timeout: 120.0
    ssl_verify: false

  gpt-4-turbo:
    provider: ollama
    endpoint: "http://localhost:11434"
    actual_model_name: "gpt-oss:20b"
    timeout: 120.0
    ssl_verify: false

  gpt-3.5-turbo:
    provider: ollama
    endpoint: "http://localhost:11434"
    actual_model_name: "gpt-oss:20b"
    timeout: 120.0
    ssl_verify: false

# No retry config needed for local Ollama
default_retry_config:
  max_retries: 1
  retry_status_codes: [500, 502, 503]
  backoff_factor: 1.0
  initial_delay: 0.5
  max_delay: 5.0

# Header manipulation
header_rules:
  # Don't drop headers - let them pass through
  drop_all: false
  drop_headers: []
  add_headers: {}
  force_headers: {}

# No transformations needed
transformations: []

# Notes:
# - All Claude and OpenAI model requests will be routed to local Ollama
# - The actual model name sent to Ollama will be "gpt-oss:20b"
# - Adjust timeout values if your local machine is slow
# - Enable log_requests to debug what Claude Code is sending
# - You can change "gpt-oss:20b" to any other Ollama model you have installed
#
# Alternative models you might want to try:
# - llama3:70b (if you have enough RAM/VRAM)
# - llama3:8b (faster, less accurate)
# - codellama:34b (optimized for code)
# - mistral:latest
# - mixtral:8x7b
#
# To use a different model, replace "gpt-oss:20b" with your preferred model
# in all the actual_model_name fields above.
